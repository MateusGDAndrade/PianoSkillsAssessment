{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z16QotNMDt9E",
        "outputId": "c65b8355-1b75-4853-ff63-17408eb684f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=169RiUEtrp4cD1zN-AjEWvSUyGOAcDgXb\n",
            "From (redirected): https://drive.google.com/uc?id=169RiUEtrp4cD1zN-AjEWvSUyGOAcDgXb&confirm=t&uuid=fd8eed84-9b56-4f9f-b289-67dd76bfaa9f\n",
            "To: c:\\Users\\luqui\\OneDrive\\lvcas\\USP\\7o Período\\Redes Neurais\\trabalho\\PianoSkillsAssessment\\processed_data.zip\n",
            "\n",
            "  0%|          | 0.00/327M [00:00<?, ?B/s]\n",
            "  0%|          | 524k/327M [00:00<02:29, 2.19MB/s]\n",
            "  1%|          | 2.62M/327M [00:00<00:37, 8.64MB/s]\n",
            "  3%|▎         | 8.39M/327M [00:00<00:13, 24.3MB/s]\n",
            "  4%|▍         | 14.7M/327M [00:00<00:08, 35.6MB/s]\n",
            "  6%|▋         | 21.0M/327M [00:00<00:07, 42.8MB/s]\n",
            "  8%|▊         | 27.3M/327M [00:00<00:06, 48.6MB/s]\n",
            " 11%|█         | 36.7M/327M [00:00<00:04, 61.3MB/s]\n",
            " 14%|█▎        | 44.6M/327M [00:00<00:04, 65.9MB/s]\n",
            " 16%|█▌        | 52.4M/327M [00:01<00:04, 67.5MB/s]\n",
            " 18%|█▊        | 60.3M/327M [00:01<00:03, 70.1MB/s]\n",
            " 21%|██        | 68.2M/327M [00:01<00:03, 72.1MB/s]\n",
            " 23%|██▎       | 75.5M/327M [00:01<00:03, 66.1MB/s]\n",
            " 25%|██▌       | 83.4M/327M [00:01<00:03, 69.2MB/s]\n",
            " 28%|██▊       | 91.2M/327M [00:01<00:03, 71.0MB/s]\n",
            " 30%|███       | 98.6M/327M [00:01<00:03, 70.6MB/s]\n",
            " 33%|███▎      | 106M/327M [00:01<00:03, 72.7MB/s] \n",
            " 35%|███▍      | 114M/327M [00:01<00:02, 71.7MB/s]\n",
            " 37%|███▋      | 121M/327M [00:02<00:02, 70.9MB/s]\n",
            " 39%|███▉      | 129M/327M [00:02<00:02, 72.1MB/s]\n",
            " 42%|████▏     | 137M/327M [00:02<00:02, 73.2MB/s]\n",
            " 44%|████▍     | 145M/327M [00:02<00:02, 73.6MB/s]\n",
            " 47%|████▋     | 153M/327M [00:02<00:02, 70.6MB/s]\n",
            " 49%|████▉     | 160M/327M [00:02<00:02, 72.0MB/s]\n",
            " 51%|█████▏    | 168M/327M [00:02<00:02, 71.9MB/s]\n",
            " 54%|█████▎    | 176M/327M [00:02<00:02, 73.2MB/s]\n",
            " 56%|█████▌    | 184M/327M [00:02<00:01, 74.0MB/s]\n",
            " 59%|█████▊    | 191M/327M [00:03<00:01, 74.5MB/s]\n",
            " 61%|██████    | 199M/327M [00:03<00:01, 74.5MB/s]\n",
            " 63%|██████▎   | 207M/327M [00:03<00:01, 74.9MB/s]\n",
            " 66%|██████▌   | 215M/327M [00:03<00:01, 75.5MB/s]\n",
            " 68%|██████▊   | 223M/327M [00:03<00:01, 74.6MB/s]\n",
            " 71%|███████   | 231M/327M [00:03<00:01, 68.8MB/s]\n",
            " 73%|███████▎  | 238M/327M [00:03<00:01, 60.3MB/s]\n",
            " 75%|███████▍  | 244M/327M [00:03<00:01, 60.2MB/s]\n",
            " 77%|███████▋  | 252M/327M [00:03<00:01, 64.6MB/s]\n",
            " 80%|███████▉  | 260M/327M [00:04<00:00, 67.2MB/s]\n",
            " 82%|████████▏ | 267M/327M [00:04<00:00, 68.8MB/s]\n",
            " 84%|████████▍ | 275M/327M [00:04<00:00, 70.9MB/s]\n",
            " 87%|████████▋ | 283M/327M [00:04<00:00, 72.6MB/s]\n",
            " 89%|████████▉ | 291M/327M [00:04<00:00, 73.9MB/s]\n",
            " 91%|█████████▏| 299M/327M [00:04<00:00, 73.8MB/s]\n",
            " 94%|█████████▍| 307M/327M [00:04<00:00, 73.7MB/s]\n",
            " 96%|█████████▌| 315M/327M [00:04<00:00, 69.5MB/s]\n",
            " 99%|█████████▊| 322M/327M [00:04<00:00, 71.6MB/s]\n",
            "100%|██████████| 327M/327M [00:04<00:00, 66.0MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Be4s_PFa2Uyc67iOqAEyMc_Ce5WiJ_fH\n",
            "From (redirected): https://drive.google.com/uc?id=1Be4s_PFa2Uyc67iOqAEyMc_Ce5WiJ_fH&confirm=t&uuid=747594ac-6943-4fb9-a549-29e740b6c59d\n",
            "To: c:\\Users\\luqui\\OneDrive\\lvcas\\USP\\7o Período\\Redes Neurais\\trabalho\\PianoSkillsAssessment\\checkpoints_batch4.zip\n",
            "\n",
            "  0%|          | 0.00/124M [00:00<?, ?B/s]\n",
            "  0%|          | 524k/124M [00:00<00:58, 2.10MB/s]\n",
            "  2%|▏         | 2.10M/124M [00:00<00:18, 6.78MB/s]\n",
            "  6%|▋         | 7.86M/124M [00:00<00:04, 23.5MB/s]\n",
            " 12%|█▏        | 15.2M/124M [00:00<00:02, 39.5MB/s]\n",
            " 18%|█▊        | 22.5M/124M [00:00<00:02, 50.1MB/s]\n",
            " 24%|██▍       | 30.4M/124M [00:00<00:01, 58.7MB/s]\n",
            " 31%|███       | 38.3M/124M [00:00<00:01, 64.1MB/s]\n",
            " 36%|███▌      | 45.1M/124M [00:00<00:01, 64.2MB/s]\n",
            " 43%|████▎     | 53.0M/124M [00:01<00:01, 67.7MB/s]\n",
            " 48%|████▊     | 60.3M/124M [00:01<00:00, 69.1MB/s]\n",
            " 54%|█████▍    | 67.6M/124M [00:01<00:00, 67.3MB/s]\n",
            " 62%|██████▏   | 76.5M/124M [00:01<00:00, 73.5MB/s]\n",
            " 68%|██████▊   | 84.4M/124M [00:01<00:00, 74.7MB/s]\n",
            " 74%|███████▍  | 92.3M/124M [00:01<00:00, 75.1MB/s]\n",
            " 80%|████████  | 100M/124M [00:01<00:00, 75.2MB/s] \n",
            " 87%|████████▋ | 108M/124M [00:01<00:00, 75.2MB/s]\n",
            " 93%|█████████▎| 116M/124M [00:01<00:00, 75.4MB/s]\n",
            " 99%|█████████▉| 124M/124M [00:02<00:00, 73.5MB/s]\n",
            "100%|██████████| 124M/124M [00:02<00:00, 61.3MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1fc6DWa9hnxYtRhuHntdXllPrrVQ54NiH\n",
            "From (redirected): https://drive.google.com/uc?id=1fc6DWa9hnxYtRhuHntdXllPrrVQ54NiH&confirm=t&uuid=87279abe-353c-440b-b446-6538734f4536\n",
            "To: c:\\Users\\luqui\\OneDrive\\lvcas\\USP\\7o Período\\Redes Neurais\\trabalho\\PianoSkillsAssessment\\audio_samples.zip\n",
            "\n",
            "  0%|          | 0.00/777M [00:00<?, ?B/s]\n",
            "  0%|          | 3.15M/777M [00:00<00:25, 30.6MB/s]\n",
            "  1%|▏         | 9.96M/777M [00:00<00:15, 50.5MB/s]\n",
            "  2%|▏         | 16.8M/777M [00:00<00:13, 57.1MB/s]\n",
            "  3%|▎         | 23.6M/777M [00:00<00:12, 59.8MB/s]\n",
            "  4%|▍         | 30.4M/777M [00:00<00:12, 61.4MB/s]\n",
            "  5%|▍         | 37.2M/777M [00:00<00:11, 62.9MB/s]\n",
            "  6%|▌         | 45.6M/777M [00:00<00:10, 66.8MB/s]\n",
            "  7%|▋         | 54.5M/777M [00:00<00:09, 72.8MB/s]\n",
            "  8%|▊         | 62.9M/777M [00:00<00:09, 74.8MB/s]\n",
            "  9%|▉         | 70.8M/777M [00:01<00:09, 75.7MB/s]\n",
            " 10%|█         | 78.6M/777M [00:01<00:09, 73.3MB/s]\n",
            " 11%|█         | 87.0M/777M [00:01<00:09, 74.9MB/s]\n",
            " 12%|█▏        | 95.4M/777M [00:01<00:08, 76.6MB/s]\n",
            " 13%|█▎        | 104M/777M [00:01<00:08, 77.5MB/s] \n",
            " 14%|█▍        | 112M/777M [00:01<00:08, 77.6MB/s]\n",
            " 16%|█▌        | 121M/777M [00:01<00:08, 79.0MB/s]\n",
            " 17%|█▋        | 129M/777M [00:01<00:08, 79.8MB/s]\n",
            " 18%|█▊        | 137M/777M [00:01<00:08, 79.6MB/s]\n",
            " 19%|█▊        | 146M/777M [00:02<00:07, 79.4MB/s]\n",
            " 20%|█▉        | 154M/777M [00:02<00:07, 79.7MB/s]\n",
            " 21%|██        | 163M/777M [00:02<00:07, 79.2MB/s]\n",
            " 22%|██▏       | 171M/777M [00:02<00:07, 79.7MB/s]\n",
            " 23%|██▎       | 179M/777M [00:02<00:07, 77.5MB/s]\n",
            " 24%|██▍       | 187M/777M [00:02<00:07, 77.0MB/s]\n",
            " 25%|██▌       | 196M/777M [00:02<00:07, 78.2MB/s]\n",
            " 26%|██▌       | 204M/777M [00:02<00:07, 78.2MB/s]\n",
            " 27%|██▋       | 212M/777M [00:02<00:07, 77.2MB/s]\n",
            " 28%|██▊       | 220M/777M [00:02<00:07, 77.4MB/s]\n",
            " 29%|██▉       | 228M/777M [00:03<00:07, 77.2MB/s]\n",
            " 30%|███       | 236M/777M [00:03<00:06, 77.7MB/s]\n",
            " 32%|███▏      | 245M/777M [00:03<00:06, 80.1MB/s]\n",
            " 33%|███▎      | 254M/777M [00:03<00:06, 79.4MB/s]\n",
            " 34%|███▎      | 262M/777M [00:03<00:06, 80.4MB/s]\n",
            " 35%|███▍      | 271M/777M [00:03<00:06, 77.0MB/s]\n",
            " 36%|███▌      | 279M/777M [00:03<00:06, 77.5MB/s]\n",
            " 37%|███▋      | 287M/777M [00:03<00:06, 77.2MB/s]\n",
            " 38%|███▊      | 295M/777M [00:03<00:06, 77.0MB/s]\n",
            " 39%|███▉      | 304M/777M [00:04<00:06, 78.1MB/s]\n",
            " 40%|████      | 312M/777M [00:04<00:05, 79.0MB/s]\n",
            " 41%|████      | 320M/777M [00:04<00:05, 77.0MB/s]\n",
            " 42%|████▏     | 329M/777M [00:04<00:05, 78.1MB/s]\n",
            " 43%|████▎     | 337M/777M [00:04<00:05, 78.5MB/s]\n",
            " 44%|████▍     | 345M/777M [00:04<00:05, 78.2MB/s]\n",
            " 45%|████▌     | 353M/777M [00:04<00:05, 77.5MB/s]\n",
            " 46%|████▋     | 361M/777M [00:04<00:05, 78.9MB/s]\n",
            " 48%|████▊     | 370M/777M [00:04<00:05, 78.6MB/s]\n",
            " 49%|████▊     | 378M/777M [00:04<00:05, 75.1MB/s]\n",
            " 50%|████▉     | 386M/777M [00:05<00:05, 73.3MB/s]\n",
            " 51%|█████     | 395M/777M [00:05<00:04, 77.2MB/s]\n",
            " 52%|█████▏    | 403M/777M [00:05<00:04, 76.9MB/s]\n",
            " 53%|█████▎    | 411M/777M [00:05<00:04, 78.1MB/s]\n",
            " 54%|█████▍    | 419M/777M [00:05<00:04, 77.8MB/s]\n",
            " 55%|█████▍    | 427M/777M [00:05<00:04, 79.1MB/s]\n",
            " 56%|█████▌    | 436M/777M [00:05<00:04, 79.2MB/s]\n",
            " 57%|█████▋    | 444M/777M [00:05<00:04, 80.1MB/s]\n",
            " 58%|█████▊    | 452M/777M [00:05<00:04, 77.8MB/s]\n",
            " 59%|█████▉    | 460M/777M [00:06<00:04, 77.5MB/s]\n",
            " 60%|██████    | 468M/777M [00:06<00:03, 77.7MB/s]\n",
            " 61%|██████▏   | 477M/777M [00:06<00:03, 78.9MB/s]\n",
            " 62%|██████▏   | 485M/777M [00:06<00:03, 77.0MB/s]\n",
            " 63%|██████▎   | 493M/777M [00:06<00:03, 76.7MB/s]\n",
            " 65%|██████▍   | 502M/777M [00:06<00:03, 77.9MB/s]\n",
            " 66%|██████▌   | 510M/777M [00:06<00:03, 71.7MB/s]\n",
            " 66%|██████▋   | 517M/777M [00:06<00:04, 61.0MB/s]\n",
            " 67%|██████▋   | 524M/777M [00:07<00:04, 58.4MB/s]\n",
            " 68%|██████▊   | 532M/777M [00:07<00:03, 64.5MB/s]\n",
            " 69%|██████▉   | 539M/777M [00:07<00:03, 61.5MB/s]\n",
            " 70%|███████   | 547M/777M [00:07<00:03, 66.8MB/s]\n",
            " 71%|███████▏  | 555M/777M [00:07<00:03, 69.8MB/s]\n",
            " 72%|███████▏  | 564M/777M [00:07<00:03, 71.2MB/s]\n",
            " 74%|███████▎  | 571M/777M [00:07<00:02, 72.8MB/s]\n",
            " 75%|███████▍  | 579M/777M [00:07<00:02, 74.4MB/s]\n",
            " 76%|███████▌  | 587M/777M [00:07<00:02, 75.2MB/s]\n",
            " 77%|███████▋  | 596M/777M [00:07<00:02, 76.9MB/s]\n",
            " 78%|███████▊  | 604M/777M [00:08<00:02, 78.4MB/s]\n",
            " 79%|███████▉  | 612M/777M [00:08<00:02, 77.8MB/s]\n",
            " 80%|███████▉  | 621M/777M [00:08<00:01, 79.4MB/s]\n",
            " 81%|████████  | 629M/777M [00:08<00:01, 78.9MB/s]\n",
            " 82%|████████▏ | 638M/777M [00:08<00:01, 75.3MB/s]\n",
            " 83%|████████▎ | 646M/777M [00:08<00:01, 77.3MB/s]\n",
            " 84%|████████▍ | 654M/777M [00:08<00:01, 78.5MB/s]\n",
            " 85%|████████▌ | 663M/777M [00:08<00:01, 78.4MB/s]\n",
            " 86%|████████▋ | 671M/777M [00:08<00:01, 78.2MB/s]\n",
            " 87%|████████▋ | 679M/777M [00:09<00:01, 79.3MB/s]\n",
            " 88%|████████▊ | 688M/777M [00:09<00:01, 79.7MB/s]\n",
            " 90%|████████▉ | 696M/777M [00:09<00:01, 79.6MB/s]\n",
            " 91%|█████████ | 705M/777M [00:09<00:00, 77.6MB/s]\n",
            " 92%|█████████▏| 713M/777M [00:09<00:00, 77.5MB/s]\n",
            " 93%|█████████▎| 721M/777M [00:09<00:00, 77.6MB/s]\n",
            " 94%|█████████▍| 729M/777M [00:09<00:00, 78.8MB/s]\n",
            " 95%|█████████▍| 738M/777M [00:09<00:00, 79.9MB/s]\n",
            " 96%|█████████▌| 746M/777M [00:09<00:00, 78.6MB/s]\n",
            " 97%|█████████▋| 754M/777M [00:09<00:00, 79.9MB/s]\n",
            " 98%|█████████▊| 763M/777M [00:10<00:00, 79.3MB/s]\n",
            " 99%|█████████▉| 771M/777M [00:10<00:00, 80.0MB/s]\n",
            "100%|██████████| 777M/777M [00:10<00:00, 75.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Baixa os arquivos zipados do Google Drive contendo os dados\n",
        "!gdown 169RiUEtrp4cD1zN-AjEWvSUyGOAcDgXb\n",
        "!gdown 1Be4s_PFa2Uyc67iOqAEyMc_Ce5WiJ_fH\n",
        "!gdown 1fc6DWa9hnxYtRhuHntdXllPrrVQ54NiH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Descompacta os arquivos zip no ambiente\n",
        "!tar -xf processed_data.zip\n",
        "!tar -xf audio_samples.zip\n",
        "!tar -xf checkpoints_batch4.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIypxGc_CDKC"
      },
      "outputs": [],
      "source": [
        "# Importa todas as bibliotecas e módulos Python necessários para construir o modelo, carregar os dados, treinar e avaliar.\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from preprocessed_dataset import PreprocessedDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from dataloader_multimodal import VideoDataset\n",
        "from opts import *\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0suy4vv43R7"
      },
      "outputs": [],
      "source": [
        "# Define os caminhos para os diretórios onde os dados de treino e teste pré-processados estão localizados.\n",
        "PROCESSED_TRAIN_DIR = './processed_data/train/'\n",
        "PROCESSED_TEST_DIR = './processed_data/test/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ7tQCf4COJq"
      },
      "outputs": [],
      "source": [
        "# Esta classe define a arquitetura da rede neural, baseada em uma ResNet18 pré-treinada,\n",
        "# adaptada para realizar tanto a classificação do nível de habilidade quanto a regressão.\n",
        "class AuralSkillClassifier(nn.Module):\n",
        "    # Construtor da classe, responsável por criar uma instância sua, herdar os atributos\n",
        "    # e métodos da classe pai (nn.Module) e definir o restante dos atributos da classe\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # inicia um backbone da rede com a ResNet18 pré-treinada\n",
        "        self.backbone = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "\n",
        "        # modifica a primeira camada para receber apenas 1 canal de entrada\n",
        "        # # (pois o espectrograma está em escala de cinza)\n",
        "        self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "\n",
        "        # remove a camada de classificação original da ResNet, substituindo-a por uma nn.Identity()\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        # cria uma nova camada de classificação, conforme descrita no artigo, com 512 valores de entrada e 128 de saída,\n",
        "        # posteriormente passando por uma camada que faz a classificação de fato, com 10 valores de saída (correspondentes às classes)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=512, out_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5)\n",
        "        )\n",
        "        self.classification_head = nn.Linear(128, num_classes)\n",
        "        self.regression_head = nn.Linear(128, 1)\n",
        "        \n",
        "    # Define o fluxo de dados através da rede neural.\n",
        "    # 'x' representa o tensor de entrada (espectrograma de áudio).\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x) # passa a entrada pela rede ResNet\n",
        "        features_128 = self.classifier(features) # passa o resultado da ResNet pela camada final de classificação\n",
        "        logits_cls = self.classification_head(features_128)\n",
        "        output_reg = self.regression_head(features_128)\n",
        "\n",
        "        return logits_cls, output_reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_class_weights(dataset, num_classes=10):\n",
        "    \"\"\"\n",
        "    Calcula os pesos para cada classe com base na frequência inversa das amostras.\n",
        "    Esta função percorre o dataset uma vez para contar as ocorrências de cada classe\n",
        "    e depois calcula os pesos.\n",
        "\n",
        "    :param dataset: Uma instância do seu objeto de Dataset (ex: PreprocessedDataset).\n",
        "    :param num_classes: O número total de classes no seu problema.\n",
        "    :return: Um tensor do PyTorch de formato [num_classes] com o peso para cada classe.\n",
        "    \"\"\"\n",
        "    print(\"Iniciando o cálculo dos pesos das classes...\")\n",
        "\n",
        "    # --- PASSO 1: Contar as Amostras de Cada Classe ---\n",
        "    \n",
        "    # A forma mais eficiente de contar é usar um DataLoader para iterar sobre os dados.\n",
        "    # batch_size pode ser maior para acelerar a contagem. shuffle=False não é necessário aqui.\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "    \n",
        "    # Usaremos um Counter para armazenar as contagens de cada label.\n",
        "    class_counts = Counter()\n",
        "    \n",
        "    # Itera sobre o dataset para contar os labels\n",
        "    for batch_data in loader:\n",
        "        labels = batch_data['player_lvl']\n",
        "        # .tolist() converte o tensor de labels do lote para uma lista Python\n",
        "        class_counts.update(labels.tolist())\n",
        "\n",
        "    # Transforma o Counter em uma lista ordenada pelo índice da classe (de 0 a 9)\n",
        "    # Se uma classe não aparecer, sua contagem será 0.\n",
        "    counts = [class_counts.get(i, 0) for i in range(num_classes)]\n",
        "    print(f\"Contagem de amostras por classe: {counts}\")\n",
        "    \n",
        "\n",
        "    # --- PASSO 2: Calcular os Pesos ---\n",
        "    \n",
        "    # A fórmula é o inverso da frequência: peso = 1 / contagem\n",
        "    # Usamos uma lista para guardar os pesos calculados.\n",
        "    weights = []\n",
        "    for count in counts:\n",
        "        # Lida com o caso de uma classe não ter amostras para evitar divisão por zero\n",
        "        if count == 0:\n",
        "            weights.append(0.0)\n",
        "        else:\n",
        "            weights.append(1.0 / count)\n",
        "\n",
        "    # Converte a lista de pesos em um tensor do PyTorch do tipo float\n",
        "    weights_tensor = torch.tensor(weights, dtype=torch.float)\n",
        "    \n",
        "    # Opcional, mas recomendado: Normalizar os pesos para que a soma deles não seja muito grande,\n",
        "    # o que poderia desestabilizar o treinamento. Aqui, normalizamos pela soma.\n",
        "    weights_tensor = weights_tensor / weights_tensor.sum()\n",
        "    \n",
        "    print(f\"Pesos calculados para as classes: {weights_tensor}\")\n",
        "    print(\"Cálculo de pesos concluído.\")\n",
        "    \n",
        "    return weights_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzeEC47MCRfB",
        "outputId": "27af1f83-11ca-425f-9582-2d34b072fdb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usando o dispositivo: cuda\n"
          ]
        }
      ],
      "source": [
        "# Define qual dispositivo será utilizado para o processamento dos cálculos da rede neural:\n",
        "# GPU (CUDA) se ela estiver disponível, caso contrário, a CPU.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Usando o dispositivo: {device}\")\n",
        "\n",
        "# Cria uma instância do modelo e a move para o dispositivo\n",
        "model = AuralSkillClassifier(num_classes=10).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csjQ_ioWCW56",
        "outputId": "5a005adc-bded-4a95-fad9-b4a3a48d53ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset encontrado. Número de amostras: 516\n",
            "Iniciando o cálculo dos pesos das classes...\n",
            "Contagem de amostras por classe: [18, 19, 34, 16, 27, 22, 72, 89, 157, 62]\n",
            "Pesos calculados para as classes: tensor([0.1682, 0.1594, 0.0891, 0.1893, 0.1122, 0.1377, 0.0421, 0.0340, 0.0193,\n",
            "        0.0488])\n",
            "Cálculo de pesos concluído.\n"
          ]
        }
      ],
      "source": [
        "# Inicializa o dataset de treino, calcula os pesos das classes para balanceamento,\n",
        "# configura o DataLoader para carregamento em lotes e define o otimizador Adam.\n",
        "train_dataset = PreprocessedDataset(data_dir=PROCESSED_TRAIN_DIR)\n",
        "pesos_tensor = calculate_class_weights(train_dataset).to(device)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define as funções de perda que serão utilizadas para calcular os erros do modelo durante o treinamento.\n",
        "criterion_cls = nn.CrossEntropyLoss(weight=pesos_tensor)\n",
        "criterion_reg_l1 = nn.L1Loss()\n",
        "criterion_reg_l2 = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6huA08HbCZjJ",
        "outputId": "664fa393-f548-41af-c247-6d8821943a8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint encontrado. Carregando o modelo pré-treinado de ./checkpoints_batch4/model_epoch_100.pt...\n",
            "Modelo carregado com sucesso a partir da época 100. Continuando o treinamento...\n"
          ]
        }
      ],
      "source": [
        "# Caminho para o modelo pré-treinado\n",
        "checkpoint_dir = './checkpoints_batch4/'\n",
        "checkpoint_path = './checkpoints_batch4/model_epoch_100.pt'\n",
        "\n",
        "# Tentar carregar o modelo e o otimizador a partir do checkpoint\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"Checkpoint encontrado. Carregando o modelo pré-treinado de {checkpoint_path}...\")\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']  # Perda final do último lote da época\n",
        "    print(f\"Modelo carregado com sucesso a partir da época {start_epoch}. Continuando o treinamento...\")\n",
        "else:\n",
        "    print(\"Nenhum checkpoint encontrado. Iniciando treinamento do zero.\")\n",
        "    start_epoch = 0  # Começar do início\n",
        "\n",
        "# --- INÍCIO DO TREINAMENTO ---\n",
        "\n",
        "# Coloque o modelo no modo de treinamento\n",
        "model.train()\n",
        "\n",
        "# Número de épocas de treinamento\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    print(f\"--- Iniciando Época {epoch+1}/{num_epochs} ---\")\n",
        "\n",
        "    # Para cada lote de dados\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "        print(f\"  Processando lote {i+1}/{len(train_loader)}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        spectrograms_tensor = batch_data['audio'].to(device)\n",
        "        labels = batch_data['player_lvl'].to(device)\n",
        "\n",
        "        lista_outputs_cls = []\n",
        "        lista_outputs_reg = []\n",
        "\n",
        "        labels_long = labels.long()\n",
        "        labels_float = labels.float()\n",
        "\n",
        "        # Processando cada clipe\n",
        "        for i in range(nclips):\n",
        "            clip_tensor = spectrograms_tensor[:, i, :, :, :]\n",
        "            logits_cls_clip, output_reg_clip = model(clip_tensor)\n",
        "            lista_outputs_cls.append(logits_cls_clip)\n",
        "            lista_outputs_reg.append(output_reg_clip)\n",
        "\n",
        "        # Calculando a saída média\n",
        "        logits_cls = torch.stack(lista_outputs_cls).mean(dim=0)\n",
        "        output_reg = torch.stack(lista_outputs_reg).mean(dim=0)\n",
        "\n",
        "        # Calculando a perda\n",
        "        loss_cls = criterion_cls(logits_cls, labels_long)\n",
        "        output_reg = output_reg.squeeze()\n",
        "        l1 = criterion_reg_l1(output_reg, labels_float)\n",
        "        l2 = criterion_reg_l2(output_reg, labels_float)\n",
        "        loss_reg = l1 + l2\n",
        "\n",
        "        # Perda total\n",
        "        loss = (1.0 * loss_cls) + (0.1 * loss_reg)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # --- Bloco de Salvamento ao Final de Cada Época ---\n",
        "    print(f\"--- Fim da Época {epoch+1}. Salvando checkpoint... ---\")\n",
        "\n",
        "    # Crie uma pasta para os checkpoints no seu Drive\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Defina o caminho completo para o arquivo do checkpoint\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pt')\n",
        "\n",
        "    # Crie o dicionário com tudo que você quer salvar\n",
        "    torch.save({\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,  # Salva a perda do último lote da época\n",
        "    }, checkpoint_path)\n",
        "\n",
        "    print(f\"Checkpoint salvo em: {checkpoint_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqsF-c1XFO6l",
        "outputId": "8cba8074-f235-478e-933b-19d03edfd4c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset encontrado. Número de amostras: 476\n",
            "Dataset de teste carregado. Total de amostras: 476\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "========================================\n",
            "--- RESULTADOS DA AVALIAÇÃO NO CONJUNTO DE TESTE ---\n",
            "========================================\n",
            "Accuracy: 60.50%\n",
            "Precision (weighted): 79.26%\n",
            "Recall (weighted): 60.50%\n",
            "F1 Score (weighted): 62.22%\n",
            "Mean Average Error (MAE): 1.14\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Garante que o modelo está em modo de avaliação (desliga dropout, etc.)\n",
        "model.eval()\n",
        "\n",
        "# Cria o dataset de teste para pegar uma amostra\n",
        "# (Certifique-se que o modo 'test' corresponde ao seu arquivo .pkl de teste)\n",
        "try:\n",
        "    test_dataset = PreprocessedDataset(PROCESSED_TEST_DIR)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, num_workers=4)\n",
        "    print(f\"Dataset de teste carregado. Total de amostras: {len(test_dataset)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao carregar o dataset de teste: {e}\")\n",
        "    print(\"Verifique se o arquivo 'annotations_unidist_test.pkl' existe na sua pasta de anotações.\")\n",
        "    # Se der erro aqui, pare a execução da célula\n",
        "    raise\n",
        "\n",
        "# Armazenar as previsões e os rótulos verdadeiros\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "# Loop para avaliar todo o dataset de teste\n",
        "for batch_data in test_loader:\n",
        "    # Prepara os dados para o modelo\n",
        "    input_tensor = batch_data['audio'].to(device)\n",
        "    true_labels_batch = batch_data['player_lvl']\n",
        "    # input_tensor = batch_data['audio'].unsqueeze(0).to(device)\n",
        "    # true_label = batch_data['player_lvl']\n",
        "\n",
        "    # Bloco de inferência sem cálculo de gradientes para economizar memória e ser mais rápido\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # --- Lógica de inferência para múltiplos clipes (mesma do treino) ---\n",
        "        clip_outputs_cls = []\n",
        "        n_clips_from_tensor = input_tensor.shape[1] # Pega o nclips do próprio tensor\n",
        "\n",
        "        for i in range(n_clips_from_tensor):\n",
        "            # Pega o i-ésimo clipe\n",
        "            clip_tensor = input_tensor[:, i, :, :, :]\n",
        "\n",
        "            # Passa o clipe pelo modelo\n",
        "            logits_cls_clip, _ = model(clip_tensor)\n",
        "\n",
        "            # Guarda a saída de classificação\n",
        "            clip_outputs_cls.append(logits_cls_clip)\n",
        "\n",
        "        # Agrega as saídas dos clipes tirando a média\n",
        "        final_logits = torch.stack(clip_outputs_cls).mean(dim=0)\n",
        "        # --- Fim da lógica de inferência ---\n",
        "\n",
        "        # Converte os logits em probabilidades\n",
        "        probabilities = torch.softmax(final_logits, dim=1)\n",
        "\n",
        "        # Pega a previsão com a maior probabilidade\n",
        "        # predicted_index = torch.argmax(probabilities, dim=1).item()\n",
        "        predicted_index = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "\n",
        "    # Armazena as previsões e os rótulos verdadeiros\n",
        "    all_predictions.extend(predicted_index.cpu().numpy())\n",
        "    all_true_labels.extend(true_labels_batch.cpu().numpy())\n",
        "\n",
        "# --- Cálculo das Métricas ---\n",
        "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
        "precision = precision_score(all_true_labels, all_predictions, average='weighted', zero_division=1)\n",
        "recall = recall_score(all_true_labels, all_predictions, average='weighted', zero_division=1)\n",
        "f1 = f1_score(all_true_labels, all_predictions, average='weighted', zero_division=1)\n",
        "mae = mean_absolute_error(all_true_labels, all_predictions)\n",
        "\n",
        "# Exibe as métricas\n",
        "print(\"=\"*40)\n",
        "print(f\"--- RESULTADOS DA AVALIAÇÃO NO CONJUNTO DE TESTE ---\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "print(f\"Precision (weighted): {precision*100:.2f}%\")\n",
        "print(f\"Recall (weighted): {recall*100:.2f}%\")\n",
        "print(f\"F1 Score (weighted): {f1*100:.2f}%\")\n",
        "print(f\"Mean Average Error (MAE): {mae:.2f}\")\n",
        "print(\"=\"*40)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
