{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esta versão usa a rede ResNet34 para verificar os resultados comparando-se com a rede ResNet18, usada no artigo original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z16QotNMDt9E",
        "outputId": "c65b8355-1b75-4853-ff63-17408eb684f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=169RiUEtrp4cD1zN-AjEWvSUyGOAcDgXb\n",
            "From (redirected): https://drive.google.com/uc?id=169RiUEtrp4cD1zN-AjEWvSUyGOAcDgXb&confirm=t&uuid=c1eb8eba-32f8-4ba6-a9b8-ed1f959071b4\n",
            "To: c:\\Users\\luqui\\OneDrive\\lvcas\\USP\\7o Período\\Redes Neurais\\trabalho\\PianoSkillsAssessment\\processed_data.zip\n",
            "\n",
            "  0%|          | 0.00/327M [00:00<?, ?B/s]\n",
            "  0%|          | 524k/327M [00:00<02:32, 2.14MB/s]\n",
            "  1%|          | 2.10M/327M [00:00<00:46, 7.02MB/s]\n",
            "  3%|▎         | 8.39M/327M [00:00<00:12, 25.4MB/s]\n",
            "  5%|▍         | 15.2M/327M [00:00<00:07, 39.0MB/s]\n",
            "  7%|▋         | 22.5M/327M [00:00<00:06, 49.6MB/s]\n",
            "  9%|▊         | 28.3M/327M [00:00<00:05, 51.1MB/s]\n",
            " 11%|█         | 35.7M/327M [00:00<00:05, 57.2MB/s]\n",
            " 13%|█▎        | 42.5M/327M [00:00<00:04, 59.3MB/s]\n",
            " 15%|█▌        | 49.8M/327M [00:01<00:04, 63.5MB/s]\n",
            " 17%|█▋        | 57.1M/327M [00:01<00:04, 64.3MB/s]\n",
            " 20%|█▉        | 65.0M/327M [00:01<00:03, 67.3MB/s]\n",
            " 22%|██▏       | 72.9M/327M [00:01<00:03, 69.1MB/s]\n",
            " 25%|██▍       | 80.2M/327M [00:01<00:03, 69.6MB/s]\n",
            " 27%|██▋       | 88.1M/327M [00:01<00:03, 70.7MB/s]\n",
            " 29%|██▉       | 95.9M/327M [00:01<00:03, 71.8MB/s]\n",
            " 32%|███▏      | 104M/327M [00:01<00:03, 72.6MB/s] \n",
            " 34%|███▍      | 112M/327M [00:01<00:02, 73.1MB/s]\n",
            " 36%|███▋      | 119M/327M [00:02<00:02, 72.9MB/s]\n",
            " 39%|███▉      | 127M/327M [00:02<00:02, 73.3MB/s]\n",
            " 41%|████      | 135M/327M [00:02<00:02, 73.7MB/s]\n",
            " 44%|████▎     | 143M/327M [00:02<00:02, 73.9MB/s]\n",
            " 46%|████▌     | 150M/327M [00:02<00:02, 73.8MB/s]\n",
            " 48%|████▊     | 158M/327M [00:02<00:02, 73.4MB/s]\n",
            " 51%|█████     | 166M/327M [00:02<00:02, 73.8MB/s]\n",
            " 53%|█████▎    | 174M/327M [00:02<00:02, 74.1MB/s]\n",
            " 56%|█████▌    | 182M/327M [00:02<00:01, 73.9MB/s]\n",
            " 58%|█████▊    | 190M/327M [00:02<00:01, 74.2MB/s]\n",
            " 60%|██████    | 198M/327M [00:03<00:01, 74.2MB/s]\n",
            " 63%|██████▎   | 206M/327M [00:03<00:01, 74.0MB/s]\n",
            " 65%|██████▌   | 213M/327M [00:03<00:01, 73.8MB/s]\n",
            " 68%|██████▊   | 221M/327M [00:03<00:01, 73.9MB/s]\n",
            " 70%|███████   | 229M/327M [00:03<00:01, 73.8MB/s]\n",
            " 72%|███████▏  | 237M/327M [00:03<00:01, 73.4MB/s]\n",
            " 75%|███████▍  | 245M/327M [00:03<00:01, 73.7MB/s]\n",
            " 77%|███████▋  | 253M/327M [00:03<00:01, 72.1MB/s]\n",
            " 80%|███████▉  | 261M/327M [00:03<00:00, 72.7MB/s]\n",
            " 82%|████████▏ | 268M/327M [00:04<00:00, 72.8MB/s]\n",
            " 85%|████████▍ | 276M/327M [00:04<00:00, 73.8MB/s]\n",
            " 87%|████████▋ | 284M/327M [00:04<00:00, 74.1MB/s]\n",
            " 89%|████████▉ | 292M/327M [00:04<00:00, 73.9MB/s]\n",
            " 92%|█████████▏| 300M/327M [00:04<00:00, 73.8MB/s]\n",
            " 94%|█████████▍| 308M/327M [00:04<00:00, 74.2MB/s]\n",
            " 97%|█████████▋| 316M/327M [00:04<00:00, 74.0MB/s]\n",
            " 99%|█████████▉| 323M/327M [00:04<00:00, 74.2MB/s]\n",
            "100%|██████████| 327M/327M [00:04<00:00, 67.4MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=19Maxl-Vh3ouFk9gqQgHO2y9T18UomC8Y\n",
            "From (redirected): https://drive.google.com/uc?id=19Maxl-Vh3ouFk9gqQgHO2y9T18UomC8Y&confirm=t&uuid=6a15e7d2-b7b8-4217-bdf1-c15f25daf920\n",
            "To: c:\\Users\\luqui\\OneDrive\\lvcas\\USP\\7o Período\\Redes Neurais\\trabalho\\PianoSkillsAssessment\\checkpoint_resnet34.zip\n",
            "\n",
            "  0%|          | 0.00/237M [00:00<?, ?B/s]\n",
            "  0%|          | 524k/237M [00:00<00:51, 4.60MB/s]\n",
            "  1%|          | 1.57M/237M [00:00<00:33, 6.94MB/s]\n",
            "  3%|▎         | 6.82M/237M [00:00<00:09, 25.5MB/s]\n",
            "  6%|▌         | 14.7M/237M [00:00<00:04, 44.5MB/s]\n",
            "  9%|▉         | 21.0M/237M [00:00<00:04, 50.5MB/s]\n",
            " 12%|█▏        | 28.3M/237M [00:00<00:03, 57.4MB/s]\n",
            " 15%|█▌        | 35.7M/237M [00:00<00:03, 62.2MB/s]\n",
            " 18%|█▊        | 43.0M/237M [00:00<00:02, 65.2MB/s]\n",
            " 21%|██        | 50.3M/237M [00:00<00:02, 67.3MB/s]\n",
            " 25%|██▍       | 58.2M/237M [00:01<00:02, 69.6MB/s]\n",
            " 28%|██▊       | 66.1M/237M [00:01<00:02, 70.4MB/s]\n",
            " 31%|███       | 73.9M/237M [00:01<00:02, 71.5MB/s]\n",
            " 34%|███▍      | 81.8M/237M [00:01<00:02, 72.3MB/s]\n",
            " 38%|███▊      | 89.7M/237M [00:01<00:02, 72.3MB/s]\n",
            " 41%|████      | 97.5M/237M [00:01<00:01, 73.1MB/s]\n",
            " 44%|████▍     | 105M/237M [00:01<00:01, 73.0MB/s] \n",
            " 48%|████▊     | 113M/237M [00:01<00:01, 73.2MB/s]\n",
            " 51%|█████     | 120M/237M [00:01<00:01, 73.2MB/s]\n",
            " 54%|█████▎    | 127M/237M [00:02<00:01, 73.1MB/s]\n",
            " 57%|█████▋    | 135M/237M [00:02<00:01, 72.5MB/s]\n",
            " 60%|██████    | 143M/237M [00:02<00:01, 71.6MB/s]\n",
            " 63%|██████▎   | 150M/237M [00:02<00:01, 72.8MB/s]\n",
            " 67%|██████▋   | 158M/237M [00:02<00:01, 69.8MB/s]\n",
            " 70%|██████▉   | 165M/237M [00:02<00:01, 68.8MB/s]\n",
            " 73%|███████▎  | 172M/237M [00:02<00:00, 66.5MB/s]\n",
            " 76%|███████▌  | 180M/237M [00:02<00:00, 68.2MB/s]\n",
            " 80%|███████▉  | 189M/237M [00:02<00:00, 73.5MB/s]\n",
            " 83%|████████▎ | 197M/237M [00:02<00:00, 71.9MB/s]\n",
            " 86%|████████▌ | 204M/237M [00:03<00:00, 72.5MB/s]\n",
            " 90%|████████▉ | 212M/237M [00:03<00:00, 72.8MB/s]\n",
            " 93%|█████████▎| 220M/237M [00:03<00:00, 73.1MB/s]\n",
            " 96%|█████████▌| 228M/237M [00:03<00:00, 73.3MB/s]\n",
            " 99%|█████████▉| 235M/237M [00:03<00:00, 72.8MB/s]\n",
            "100%|██████████| 237M/237M [00:03<00:00, 67.0MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1fc6DWa9hnxYtRhuHntdXllPrrVQ54NiH\n",
            "From (redirected): https://drive.google.com/uc?id=1fc6DWa9hnxYtRhuHntdXllPrrVQ54NiH&confirm=t&uuid=081a09f0-5586-4bf6-9bb1-2aa07d791892\n",
            "To: c:\\Users\\luqui\\OneDrive\\lvcas\\USP\\7o Período\\Redes Neurais\\trabalho\\PianoSkillsAssessment\\audio_samples.zip\n",
            "\n",
            "  0%|          | 0.00/777M [00:00<?, ?B/s]\n",
            "  0%|          | 524k/777M [00:00<06:38, 1.95MB/s]\n",
            "  0%|          | 2.10M/777M [00:00<01:57, 6.58MB/s]\n",
            "  1%|          | 7.86M/777M [00:00<00:33, 22.9MB/s]\n",
            "  2%|▏         | 13.1M/777M [00:00<00:26, 28.6MB/s]\n",
            "  3%|▎         | 21.0M/777M [00:00<00:18, 41.8MB/s]\n",
            "  4%|▎         | 28.3M/777M [00:00<00:15, 49.9MB/s]\n",
            "  5%|▍         | 36.2M/777M [00:00<00:13, 56.3MB/s]\n",
            "  6%|▌         | 44.0M/777M [00:01<00:12, 60.8MB/s]\n",
            "  7%|▋         | 51.9M/777M [00:01<00:11, 63.8MB/s]\n",
            "  8%|▊         | 59.8M/777M [00:01<00:10, 65.9MB/s]\n",
            "  9%|▊         | 67.6M/777M [00:01<00:10, 67.1MB/s]\n",
            " 10%|▉         | 75.0M/777M [00:01<00:10, 67.7MB/s]\n",
            " 11%|█         | 82.8M/777M [00:01<00:10, 68.7MB/s]\n",
            " 12%|█▏        | 90.7M/777M [00:01<00:09, 69.4MB/s]\n",
            " 13%|█▎        | 98.6M/777M [00:01<00:09, 70.0MB/s]\n",
            " 14%|█▎        | 106M/777M [00:01<00:09, 70.2MB/s] \n",
            " 15%|█▍        | 114M/777M [00:02<00:09, 70.0MB/s]\n",
            " 16%|█▌        | 122M/777M [00:02<00:09, 69.7MB/s]\n",
            " 17%|█▋        | 129M/777M [00:02<00:09, 70.3MB/s]\n",
            " 18%|█▊        | 137M/777M [00:02<00:09, 70.3MB/s]\n",
            " 19%|█▊        | 145M/777M [00:02<00:08, 70.4MB/s]\n",
            " 20%|█▉        | 153M/777M [00:02<00:08, 70.5MB/s]\n",
            " 21%|██        | 160M/777M [00:02<00:08, 70.2MB/s]\n",
            " 22%|██▏       | 168M/777M [00:02<00:08, 70.0MB/s]\n",
            " 23%|██▎       | 176M/777M [00:02<00:08, 70.7MB/s]\n",
            " 24%|██▎       | 184M/777M [00:03<00:08, 71.0MB/s]\n",
            " 25%|██▍       | 192M/777M [00:03<00:08, 71.0MB/s]\n",
            " 26%|██▌       | 200M/777M [00:03<00:08, 70.4MB/s]\n",
            " 27%|██▋       | 207M/777M [00:03<00:08, 70.5MB/s]\n",
            " 28%|██▊       | 215M/777M [00:03<00:07, 70.9MB/s]\n",
            " 29%|██▊       | 223M/777M [00:03<00:07, 70.8MB/s]\n",
            " 30%|██▉       | 231M/777M [00:03<00:08, 68.3MB/s]\n",
            " 31%|███       | 239M/777M [00:03<00:07, 69.5MB/s]\n",
            " 32%|███▏      | 246M/777M [00:03<00:07, 69.9MB/s]\n",
            " 33%|███▎      | 254M/777M [00:04<00:07, 70.3MB/s]\n",
            " 34%|███▎      | 262M/777M [00:04<00:07, 70.5MB/s]\n",
            " 35%|███▍      | 270M/777M [00:04<00:07, 70.1MB/s]\n",
            " 36%|███▌      | 277M/777M [00:04<00:07, 70.5MB/s]\n",
            " 37%|███▋      | 285M/777M [00:04<00:06, 70.7MB/s]\n",
            " 38%|███▊      | 293M/777M [00:04<00:06, 70.9MB/s]\n",
            " 39%|███▊      | 301M/777M [00:04<00:06, 71.0MB/s]\n",
            " 40%|███▉      | 309M/777M [00:04<00:06, 71.0MB/s]\n",
            " 41%|████      | 317M/777M [00:04<00:06, 70.7MB/s]\n",
            " 42%|████▏     | 324M/777M [00:05<00:06, 70.8MB/s]\n",
            " 43%|████▎     | 332M/777M [00:05<00:06, 70.6MB/s]\n",
            " 44%|████▎     | 340M/777M [00:05<00:06, 71.0MB/s]\n",
            " 45%|████▍     | 348M/777M [00:05<00:06, 70.3MB/s]\n",
            " 46%|████▌     | 355M/777M [00:05<00:05, 70.8MB/s]\n",
            " 47%|████▋     | 363M/777M [00:05<00:05, 70.7MB/s]\n",
            " 48%|████▊     | 371M/777M [00:05<00:05, 71.2MB/s]\n",
            " 49%|████▊     | 379M/777M [00:05<00:05, 71.0MB/s]\n",
            " 50%|████▉     | 386M/777M [00:05<00:05, 71.0MB/s]\n",
            " 51%|█████     | 394M/777M [00:06<00:05, 70.4MB/s]\n",
            " 52%|█████▏    | 402M/777M [00:06<00:05, 70.7MB/s]\n",
            " 53%|█████▎    | 409M/777M [00:06<00:05, 70.8MB/s]\n",
            " 54%|█████▎    | 417M/777M [00:06<00:05, 71.0MB/s]\n",
            " 55%|█████▍    | 425M/777M [00:06<00:05, 70.3MB/s]\n",
            " 56%|█████▌    | 433M/777M [00:06<00:04, 70.8MB/s]\n",
            " 57%|█████▋    | 440M/777M [00:06<00:04, 70.6MB/s]\n",
            " 58%|█████▊    | 448M/777M [00:06<00:04, 71.0MB/s]\n",
            " 59%|█████▊    | 456M/777M [00:06<00:04, 70.8MB/s]\n",
            " 60%|█████▉    | 464M/777M [00:07<00:04, 70.4MB/s]\n",
            " 61%|██████    | 472M/777M [00:07<00:04, 70.5MB/s]\n",
            " 62%|██████▏   | 479M/777M [00:07<00:04, 71.0MB/s]\n",
            " 63%|██████▎   | 487M/777M [00:07<00:04, 71.0MB/s]\n",
            " 64%|██████▎   | 495M/777M [00:07<00:03, 71.4MB/s]\n",
            " 65%|██████▍   | 502M/777M [00:07<00:03, 71.9MB/s]\n",
            " 66%|██████▌   | 510M/777M [00:07<00:03, 71.8MB/s]\n",
            " 66%|██████▋   | 517M/777M [00:07<00:03, 71.1MB/s]\n",
            " 67%|██████▋   | 524M/777M [00:07<00:03, 70.5MB/s]\n",
            " 68%|██████▊   | 532M/777M [00:07<00:03, 69.9MB/s]\n",
            " 69%|██████▉   | 539M/777M [00:08<00:03, 70.0MB/s]\n",
            " 70%|███████   | 546M/777M [00:08<00:03, 69.8MB/s]\n",
            " 71%|███████   | 554M/777M [00:08<00:03, 68.8MB/s]\n",
            " 72%|███████▏  | 562M/777M [00:08<00:03, 69.6MB/s]\n",
            " 73%|███████▎  | 569M/777M [00:08<00:02, 70.0MB/s]\n",
            " 74%|███████▍  | 577M/777M [00:08<00:02, 70.5MB/s]\n",
            " 75%|███████▌  | 585M/777M [00:08<00:02, 70.0MB/s]\n",
            " 76%|███████▌  | 592M/777M [00:08<00:02, 70.3MB/s]\n",
            " 77%|███████▋  | 600M/777M [00:08<00:02, 70.7MB/s]\n",
            " 78%|███████▊  | 608M/777M [00:09<00:02, 71.3MB/s]\n",
            " 79%|███████▉  | 615M/777M [00:09<00:02, 70.8MB/s]\n",
            " 80%|████████  | 623M/777M [00:09<00:02, 70.7MB/s]\n",
            " 81%|████████  | 630M/777M [00:09<00:02, 59.6MB/s]\n",
            " 82%|████████▏ | 637M/777M [00:09<00:02, 61.5MB/s]\n",
            " 83%|████████▎ | 645M/777M [00:09<00:02, 63.8MB/s]\n",
            " 84%|████████▍ | 652M/777M [00:09<00:01, 65.5MB/s]\n",
            " 85%|████████▍ | 659M/777M [00:09<00:01, 65.5MB/s]\n",
            " 86%|████████▌ | 667M/777M [00:09<00:01, 69.1MB/s]\n",
            " 87%|████████▋ | 674M/777M [00:10<00:01, 69.2MB/s]\n",
            " 88%|████████▊ | 683M/777M [00:10<00:01, 72.5MB/s]\n",
            " 89%|████████▊ | 690M/777M [00:10<00:01, 71.4MB/s]\n",
            " 90%|████████▉ | 697M/777M [00:10<00:01, 70.9MB/s]\n",
            " 91%|█████████ | 705M/777M [00:10<00:01, 70.3MB/s]\n",
            " 92%|█████████▏| 712M/777M [00:10<00:00, 69.8MB/s]\n",
            " 93%|█████████▎| 719M/777M [00:10<00:01, 54.3MB/s]\n",
            " 93%|█████████▎| 726M/777M [00:10<00:00, 54.8MB/s]\n",
            " 94%|█████████▍| 733M/777M [00:11<00:00, 59.3MB/s]\n",
            " 95%|█████████▌| 741M/777M [00:11<00:00, 62.7MB/s]\n",
            " 96%|█████████▌| 748M/777M [00:11<00:00, 61.8MB/s]\n",
            " 97%|█████████▋| 756M/777M [00:11<00:00, 66.5MB/s]\n",
            " 98%|█████████▊| 764M/777M [00:11<00:00, 67.8MB/s]\n",
            " 99%|█████████▉| 772M/777M [00:11<00:00, 68.7MB/s]\n",
            "100%|██████████| 777M/777M [00:11<00:00, 66.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Baixa os arquivos zipados do Google Drive contendo os dados (essencial para o código poder funcionar)\n",
        "\n",
        "# processed_data.zip\n",
        "!gdown 169RiUEtrp4cD1zN-AjEWvSUyGOAcDgXb\n",
        "\n",
        "# checkpoints_resnet34.zip\n",
        "!gdown 19Maxl-Vh3ouFk9gqQgHO2y9T18UomC8Y\n",
        "\n",
        "# audio_samples.zip\n",
        "!gdown 1fc6DWa9hnxYtRhuHntdXllPrrVQ54NiH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Descompacta os arquivos zip no ambiente\n",
        "!tar -xf processed_data.zip\n",
        "!tar -xf audio_samples.zip\n",
        "!tar -xf checkpoints_resnet34.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fIypxGc_CDKC"
      },
      "outputs": [],
      "source": [
        "# Importa todas as bibliotecas e módulos Python necessários para construir o modelo, carregar os dados, treinar e avaliar.\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from preprocessed_dataset import PreprocessedDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet34, ResNet34_Weights\n",
        "from dataloader_multimodal import VideoDataset\n",
        "from opts import *\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N0suy4vv43R7"
      },
      "outputs": [],
      "source": [
        "# Define os caminhos para os diretórios onde os dados de treino e teste pré-processados estão localizados.\n",
        "PROCESSED_TRAIN_DIR = './processed_data/train/'\n",
        "PROCESSED_TEST_DIR = './processed_data/test/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EZ7tQCf4COJq"
      },
      "outputs": [],
      "source": [
        "# Esta classe define a arquitetura da rede neural, baseada em uma ResNet34 pré-treinada,\n",
        "# adaptada para realizar tanto a classificação do nível de habilidade quanto a regressão.\n",
        "class AuralSkillClassifier(nn.Module):\n",
        "    # Construtor da classe, responsável por criar uma instância sua, herdar os atributos\n",
        "    # e métodos da classe pai (nn.Module) e definir o restante dos atributos da classe\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # inicia um backbone da rede com a ResNet34 pré-treinada\n",
        "        self.backbone = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
        "\n",
        "        # modifica a primeira camada para receber apenas 1 canal de entrada\n",
        "        # # (pois o espectrograma está em escala de cinza)\n",
        "        self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "\n",
        "        # remove a camada de classificação original da ResNet, substituindo-a por uma nn.Identity()\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        # cria uma nova camada de classificação, conforme descrita no artigo, com 512 valores de entrada e 128 de saída,\n",
        "        # posteriormente passando por uma camada que faz a classificação de fato, com 10 valores de saída (correspondentes às classes)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=512, out_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5)\n",
        "        )\n",
        "        self.classification_head = nn.Linear(128, num_classes)\n",
        "        self.regression_head = nn.Linear(128, 1)\n",
        "        \n",
        "    # Define o fluxo de dados através da rede neural.\n",
        "    # 'x' representa o tensor de entrada (espectrograma de áudio).\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x) # passa a entrada pela rede ResNet\n",
        "        features_128 = self.classifier(features) # passa o resultado da ResNet pela camada final de classificação\n",
        "        logits_cls = self.classification_head(features_128)\n",
        "        output_reg = self.regression_head(features_128)\n",
        "\n",
        "        return logits_cls, output_reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_class_weights(dataset, num_classes=10):\n",
        "    \"\"\"\n",
        "    Calcula os pesos para cada classe com base na frequência inversa das amostras.\n",
        "    Esta função percorre o dataset uma vez para contar as ocorrências de cada classe\n",
        "    e depois calcula os pesos.\n",
        "\n",
        "    :param dataset: Uma instância do seu objeto de Dataset (ex: PreprocessedDataset).\n",
        "    :param num_classes: O número total de classes no seu problema.\n",
        "    :return: Um tensor do PyTorch de formato [num_classes] com o peso para cada classe.\n",
        "    \"\"\"\n",
        "    print(\"Iniciando o cálculo dos pesos das classes...\")\n",
        "\n",
        "    # --- PASSO 1: Contar as Amostras de Cada Classe ---\n",
        "    \n",
        "    # A forma mais eficiente de contar é usar um DataLoader para iterar sobre os dados.\n",
        "    # batch_size pode ser maior para acelerar a contagem. shuffle=False não é necessário aqui.\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "    \n",
        "    # Usaremos um Counter para armazenar as contagens de cada label.\n",
        "    class_counts = Counter()\n",
        "    \n",
        "    # Itera sobre o dataset para contar os labels\n",
        "    for batch_data in loader:\n",
        "        labels = batch_data['player_lvl']\n",
        "        # .tolist() converte o tensor de labels do lote para uma lista Python\n",
        "        class_counts.update(labels.tolist())\n",
        "\n",
        "    # Transforma o Counter em uma lista ordenada pelo índice da classe (de 0 a 9)\n",
        "    # Se uma classe não aparecer, sua contagem será 0.\n",
        "    counts = [class_counts.get(i, 0) for i in range(num_classes)]\n",
        "    print(f\"Contagem de amostras por classe: {counts}\")\n",
        "    \n",
        "\n",
        "    # --- PASSO 2: Calcular os Pesos ---\n",
        "    \n",
        "    # A fórmula é o inverso da frequência: peso = 1 / contagem\n",
        "    # Usamos uma lista para guardar os pesos calculados.\n",
        "    weights = []\n",
        "    for count in counts:\n",
        "        # Lida com o caso de uma classe não ter amostras para evitar divisão por zero\n",
        "        if count == 0:\n",
        "            weights.append(0.0)\n",
        "        else:\n",
        "            weights.append(1.0 / count)\n",
        "\n",
        "    # Converte a lista de pesos em um tensor do PyTorch do tipo float\n",
        "    weights_tensor = torch.tensor(weights, dtype=torch.float)\n",
        "    \n",
        "    # Opcional, mas recomendado: Normalizar os pesos para que a soma deles não seja muito grande,\n",
        "    # o que poderia desestabilizar o treinamento. Aqui, normalizamos pela soma.\n",
        "    weights_tensor = weights_tensor / weights_tensor.sum()\n",
        "    \n",
        "    print(f\"Pesos calculados para as classes: {weights_tensor}\")\n",
        "    print(\"Cálculo de pesos concluído.\")\n",
        "    \n",
        "    return weights_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzeEC47MCRfB",
        "outputId": "27af1f83-11ca-425f-9582-2d34b072fdb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usando o dispositivo: cuda\n"
          ]
        }
      ],
      "source": [
        "# Define qual dispositivo será utilizado para o processamento dos cálculos da rede neural:\n",
        "# GPU (CUDA) se ela estiver disponível, caso contrário, a CPU.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Usando o dispositivo: {device}\")\n",
        "\n",
        "# Cria uma instância do modelo e a move para o dispositivo\n",
        "model = AuralSkillClassifier(num_classes=10).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csjQ_ioWCW56",
        "outputId": "5a005adc-bded-4a95-fad9-b4a3a48d53ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset encontrado. Número de amostras: 516\n",
            "Iniciando o cálculo dos pesos das classes...\n",
            "Contagem de amostras por classe: [18, 19, 34, 16, 27, 22, 72, 89, 157, 62]\n",
            "Pesos calculados para as classes: tensor([0.1682, 0.1594, 0.0891, 0.1893, 0.1122, 0.1377, 0.0421, 0.0340, 0.0193,\n",
            "        0.0488])\n",
            "Cálculo de pesos concluído.\n"
          ]
        }
      ],
      "source": [
        "# Inicializa o dataset de treino, calcula os pesos das classes para balanceamento,\n",
        "# configura o DataLoader para carregamento em lotes e define o otimizador Adam.\n",
        "train_dataset = PreprocessedDataset(data_dir=PROCESSED_TRAIN_DIR)\n",
        "pesos_tensor = calculate_class_weights(train_dataset).to(device)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define as funções de perda que serão utilizadas para calcular os erros do modelo durante o treinamento.\n",
        "criterion_cls = nn.CrossEntropyLoss(weight=pesos_tensor)\n",
        "criterion_reg_l1 = nn.L1Loss()\n",
        "criterion_reg_l2 = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6huA08HbCZjJ",
        "outputId": "664fa393-f548-41af-c247-6d8821943a8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint encontrado. Carregando o modelo pré-treinado de ./checkpoints_resnet34/model_epoch_100.pt...\n",
            "Modelo carregado com sucesso a partir da época 100. Continuando o treinamento...\n"
          ]
        }
      ],
      "source": [
        "# Caminho para o modelo pré-treinado\n",
        "checkpoint_dir = './checkpoints_resnet34/'\n",
        "checkpoint_path = './checkpoints_resnet34/model_epoch_100.pt'\n",
        "\n",
        "# Tentar carregar o modelo e o otimizador a partir do checkpoint\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"Checkpoint encontrado. Carregando o modelo pré-treinado de {checkpoint_path}...\")\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']  # Perda final do último lote da época\n",
        "    print(f\"Modelo carregado com sucesso a partir da época {start_epoch}. Continuando o treinamento...\")\n",
        "else:\n",
        "    print(\"Nenhum checkpoint encontrado. Iniciando treinamento do zero.\")\n",
        "    start_epoch = 0  # Começar do início\n",
        "\n",
        "# --- INÍCIO DO TREINAMENTO ---\n",
        "\n",
        "# Coloque o modelo no modo de treinamento\n",
        "model.train()\n",
        "\n",
        "# Número de épocas de treinamento\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    print(f\"--- Iniciando Época {epoch+1}/{num_epochs} ---\")\n",
        "\n",
        "    # Para cada lote de dados\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "        print(f\"  Processando lote {i+1}/{len(train_loader)}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        spectrograms_tensor = batch_data['audio'].to(device)\n",
        "        labels = batch_data['player_lvl'].to(device)\n",
        "\n",
        "        lista_outputs_cls = []\n",
        "        lista_outputs_reg = []\n",
        "\n",
        "        labels_long = labels.long()\n",
        "        labels_float = labels.float()\n",
        "\n",
        "        # Processando cada clipe\n",
        "        for i in range(nclips):\n",
        "            clip_tensor = spectrograms_tensor[:, i, :, :, :]\n",
        "            logits_cls_clip, output_reg_clip = model(clip_tensor)\n",
        "            lista_outputs_cls.append(logits_cls_clip)\n",
        "            lista_outputs_reg.append(output_reg_clip)\n",
        "\n",
        "        # Calculando a saída média\n",
        "        logits_cls = torch.stack(lista_outputs_cls).mean(dim=0)\n",
        "        output_reg = torch.stack(lista_outputs_reg).mean(dim=0)\n",
        "\n",
        "        # Calculando a perda\n",
        "        loss_cls = criterion_cls(logits_cls, labels_long)\n",
        "        output_reg = output_reg.squeeze()\n",
        "        l1 = criterion_reg_l1(output_reg, labels_float)\n",
        "        l2 = criterion_reg_l2(output_reg, labels_float)\n",
        "        loss_reg = l1 + l2\n",
        "\n",
        "        # Perda total\n",
        "        loss = (1.0 * loss_cls) + (0.1 * loss_reg)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # --- Bloco de Salvamento ao Final de Cada Época ---\n",
        "    print(f\"--- Fim da Época {epoch+1}. Salvando checkpoint... ---\")\n",
        "\n",
        "    # Crie uma pasta para os checkpoints no seu Drive\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Defina o caminho completo para o arquivo do checkpoint\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pt')\n",
        "\n",
        "    # Crie o dicionário com tudo que você quer salvar\n",
        "    torch.save({\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,  # Salva a perda do último lote da época\n",
        "    }, checkpoint_path)\n",
        "\n",
        "    print(f\"Checkpoint salvo em: {checkpoint_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqsF-c1XFO6l",
        "outputId": "8cba8074-f235-478e-933b-19d03edfd4c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset encontrado. Número de amostras: 476\n",
            "Dataset de teste carregado. Total de amostras: 476\n",
            "========================================\n",
            "--- RESULTADOS DA AVALIAÇÃO NO CONJUNTO DE TESTE ---\n",
            "========================================\n",
            "Accuracy: 62.61%\n",
            "Precision (weighted): 66.95%\n",
            "Recall (weighted): 62.61%\n",
            "F1 Score (weighted): 60.29%\n",
            "Mean Average Error (MAE): 0.67\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Garante que o modelo está em modo de avaliação (desliga dropout, etc.)\n",
        "model.eval()\n",
        "\n",
        "# Cria o dataset de teste para pegar uma amostra\n",
        "# (Certifique-se que o modo 'test' corresponde ao seu arquivo .pkl de teste)\n",
        "try:\n",
        "    test_dataset = PreprocessedDataset(PROCESSED_TEST_DIR)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, num_workers=4)\n",
        "    print(f\"Dataset de teste carregado. Total de amostras: {len(test_dataset)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao carregar o dataset de teste: {e}\")\n",
        "    print(\"Verifique se o arquivo 'annotations_unidist_test.pkl' existe na sua pasta de anotações.\")\n",
        "    # Se der erro aqui, pare a execução da célula\n",
        "    raise\n",
        "\n",
        "# Armazenar as previsões e os rótulos verdadeiros\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "# Loop para avaliar todo o dataset de teste\n",
        "for batch_data in test_loader:\n",
        "    # Prepara os dados para o modelo\n",
        "    input_tensor = batch_data['audio'].to(device)\n",
        "    true_labels_batch = batch_data['player_lvl']\n",
        "    # input_tensor = batch_data['audio'].unsqueeze(0).to(device)\n",
        "    # true_label = batch_data['player_lvl']\n",
        "\n",
        "    # Bloco de inferência sem cálculo de gradientes para economizar memória e ser mais rápido\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # --- Lógica de inferência para múltiplos clipes (mesma do treino) ---\n",
        "        clip_outputs_cls = []\n",
        "        n_clips_from_tensor = input_tensor.shape[1] # Pega o nclips do próprio tensor\n",
        "\n",
        "        for i in range(n_clips_from_tensor):\n",
        "            # Pega o i-ésimo clipe\n",
        "            clip_tensor = input_tensor[:, i, :, :, :]\n",
        "\n",
        "            # Passa o clipe pelo modelo\n",
        "            logits_cls_clip, _ = model(clip_tensor)\n",
        "\n",
        "            # Guarda a saída de classificação\n",
        "            clip_outputs_cls.append(logits_cls_clip)\n",
        "\n",
        "        # Agrega as saídas dos clipes tirando a média\n",
        "        final_logits = torch.stack(clip_outputs_cls).mean(dim=0)\n",
        "        # --- Fim da lógica de inferência ---\n",
        "\n",
        "        # Converte os logits em probabilidades\n",
        "        probabilities = torch.softmax(final_logits, dim=1)\n",
        "\n",
        "        # Pega a previsão com a maior probabilidade\n",
        "        # predicted_index = torch.argmax(probabilities, dim=1).item()\n",
        "        predicted_index = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "\n",
        "    # Armazena as previsões e os rótulos verdadeiros\n",
        "    all_predictions.extend(predicted_index.cpu().numpy())\n",
        "    all_true_labels.extend(true_labels_batch.cpu().numpy())\n",
        "\n",
        "# --- Cálculo das Métricas ---\n",
        "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
        "precision = precision_score(all_true_labels, all_predictions, average='weighted', zero_division=1)\n",
        "recall = recall_score(all_true_labels, all_predictions, average='weighted', zero_division=1)\n",
        "f1 = f1_score(all_true_labels, all_predictions, average='weighted', zero_division=1)\n",
        "mae = mean_absolute_error(all_true_labels, all_predictions)\n",
        "\n",
        "# Exibe as métricas\n",
        "print(\"=\"*40)\n",
        "print(f\"--- RESULTADOS DA AVALIAÇÃO NO CONJUNTO DE TESTE ---\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "print(f\"Precision (weighted): {precision*100:.2f}%\")\n",
        "print(f\"Recall (weighted): {recall*100:.2f}%\")\n",
        "print(f\"F1 Score (weighted): {f1*100:.2f}%\")\n",
        "print(f\"Mean Average Error (MAE): {mae:.2f}\")\n",
        "print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ao comparar as duas arquiteturas, a ResNet34 teve o melhor desempenho, chegando a uma acurácia de 62.61% contra 60.50% da ResNet18. A melhoria mais notável, no entanto, foi observada no Erro Absoluto Médio (MAE), que caiu de 1.14 para 0.67. Isso mostra que a maior profundidade da ResNet-34 permitiu não só uma classificação exata mais frequente, mas, também, uma compreensão superior da natureza ordinal das classes de habilidade, resultando em erros consideravelmente menores.\n",
        "\n",
        "A análise sugere que a ResNet34, apesar de ter precisão ponderada menor, adotou uma estratégia de aprendizado mais eficaz, chegando a um modelo final mais robusto."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
